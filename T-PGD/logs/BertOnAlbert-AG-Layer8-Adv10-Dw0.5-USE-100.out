2023-07-12 11:44:06.525578: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-12 11:44:06.578694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-12 11:44:07.954472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Found cached dataset ag_news (/home/yuling/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)
ARGS:  Namespace(task='ag_news', model_checkpoint='textattack/bert-base-uncased-ag-news', tokenizer_checkpoint='bert-base-uncased', decode_mode='Bert', victim_model_checkpoint='textattack/albert-base-v2-ag-news', victim_tokenizer_checkpoint='textattack/albert-base-v2-ag-news', data_local_path=None, cuda_device=0, victim_device=-1, start=0, end=1000, decode_layer=8, perturb_layer=0, num_seg_steps=100, num_adv_steps=20, adv_lr=10.0, init_mag=3.0, decode_weight=-0.5, bs_lower_limit=0.3, bs_upper_limit=0.95, local_victim=None, local_model=None, target_metric='use', stop_random_cover=False, eval_lower_limit=0.0)
running process on cuda: 0
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 712.71it/s]
Loading cached processed dataset at /home/yuling/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-121d6d8e8909a938.arrow
Loading cached processed dataset at /home/yuling/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-e254730a1055d493.arrow
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Stop words:  not Not not Not 's 's s S doesn Doesn doesn't Doesn't no No was Was wasn Wasn wasn't Wasn't without Without could Could couldn Couldn couldn't Couldn't is Is isn Isn isn't Isn't hadn Hadn hadn't Hadn't hasn Hasn hasn't Hasn't haven Haven haven't Haven't were Were weren Weren weren't Weren't and And but But can Can cannot Cannot
Stop ids:  [2053, 2064, 3475, 2071, 1055, 1056, 2987, 2347, 2481, 4033, 1998, 2001, 2003, 4694, 2910, 2020, 101, 2021, 3685, 102, 2025, 1005, 8440, 2302]
Traceback (most recent call last):
  File "/home/yuling/myproject/latent-adv-unfinished/T-PGD/RunTPGD.py", line 121, in <module>
    victim_tokenizer = AutoTokenizer.from_pretrained(args.victim_tokenizer_checkpoint)
  File "/home/yuling/miniconda3/envs/t-pgd/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 676, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/yuling/miniconda3/envs/t-pgd/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1804, in from_pretrained
    return cls._from_pretrained(
  File "/home/yuling/miniconda3/envs/t-pgd/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1959, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/yuling/miniconda3/envs/t-pgd/lib/python3.9/site-packages/transformers/models/albert/tokenization_albert_fast.py", line 148, in __init__
    super().__init__(
  File "/home/yuling/miniconda3/envs/t-pgd/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 120, in __init__
    raise ValueError(
ValueError: Couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
